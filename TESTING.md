# Testing Guidelines for RadioGrab Development

## ğŸ¯ Overview

This document establishes comprehensive testing requirements for all RadioGrab development work. Every code change must be thoroughly tested using real user workflows to ensure reliability and quality.

## ğŸš¨ CRITICAL TESTING REQUIREMENTS

### **ğŸ”¥ MANDATORY: Test EVERYTHING - No Exceptions**
**YOU MUST TEST ALL LINKS, ALL PAGES, AND ALL FEATURES WITHOUT EXCEPTION.**

- **Test ALL navigation menu links** - click every single link in the main menu
- **Test ALL page functionality** - every button, form, modal, dropdown, filter, search
- **Test ALL user workflows** - complete end-to-end user journeys 
- **Test ALL interactive elements** - hover effects, clicks, form submissions, validations
- **Test ALL error conditions** - invalid inputs, missing data, network failures
- **DO NOT assume API calls working means the page works** - test the actual user interface
- **DO NOT skip any pages or features** - comprehensive means COMPREHENSIVE

### **1. Always Test as a Real User Using Chrome/Chromium Browser**
- Use the actual web browser interface at https://radiograb.svaha.com
- Follow complete user workflows from start to finish  
- Never assume functionality works - verify it through actual usage
- Test using the same methods and interfaces that end users would use
- **Click every single link and button on every page**
- **Fill out and submit every form with both valid and invalid data**

### **2. Systematic Page-by-Page Testing Protocol**
- **Test immediately after implementing each feature/fix**
- **Test ALL navigation links first** - ensure every menu item loads without errors
- **Test ALL page functionality** - forms, buttons, modals, filters, search, etc.
- **Test ALL workflows end-to-end** - complete user journeys from start to finish
- **Create GitHub issues for ALL problems found** - do not fix immediately
- **Continue testing other features** - complete full testing cycle before fixing
- **Fix ALL issues only after complete testing cycle**

### **3. Production Environment Testing Requirements**
- **Always test on the live production site** (https://radiograb.svaha.com)
- **Use the deployed version, not just local development**
- **Verify changes work in the actual Docker container environment**
- **Test with real data and real user scenarios**
- **Test both authenticated and unauthenticated user flows**

## ğŸ§ª COMPREHENSIVE TESTING CHECKLIST

### **ğŸ“‹ Complete Page Testing Protocol**
**EVERY page must be tested with this exact checklist:**

1. **Page Load Test**
   - âœ… Page loads without 500/404 errors
   - âœ… All CSS and JavaScript resources load
   - âœ… No console errors in browser dev tools
   - âœ… Page renders correctly (no broken layout)

2. **Navigation Test**  
   - âœ… All navigation menu links work
   - âœ… Breadcrumbs function correctly
   - âœ… Back/forward browser buttons work
   - âœ… All page-specific links and buttons work

3. **Form Testing (if applicable)**
   - âœ… Form loads without errors
   - âœ… All form fields accept input
   - âœ… Form validation works with invalid data
   - âœ… Form submission works with valid data
   - âœ… Success/error messages display correctly
   - âœ… CSRF tokens function properly

4. **Interactive Elements**
   - âœ… All buttons clickable and functional
   - âœ… All dropdowns open and close
   - âœ… All modals open and close properly
   - âœ… All filters and search features work
   - âœ… All AJAX calls complete successfully

5. **Authentication Test**
   - âœ… Unauthenticated access works as intended
   - âœ… Authenticated access works as intended  
   - âœ… Proper redirects to login when required
   - âœ… Access control enforced correctly

### **ğŸ“‹ Page-by-Page Testing Requirements**
**Test EVERY page in this exact order:**

#### **ğŸ  Public Pages (Test Unauthenticated)**
- [ ] **Dashboard** (/) - Statistics display, links work
- [ ] **Stations** (/stations.php) - Station cards, actions, filters
- [ ] **Shows** (/shows.php) - Show listings, filtering, sorting  
- [ ] **Playlists** (/playlists.php) - Playlist management
- [ ] **RSS Feeds** (/feeds.php) - Feed listings, copy URLs, tabs
- [ ] **Browse Templates** (/browse-templates.php) - Template browsing
- [ ] **Login** (/login.php) - Form submission, validation
- [ ] **Forgot Password** (/forgot-password.php) - Password reset flow

#### **â• Form Pages (Test All Form Functions)**
- [ ] **Add Station** (/add-station.php) - Form validation, discovery, testing
- [ ] **Edit Station** (/edit-station.php?id=X) - Pre-population, updates
- [ ] **Add Show** (/add-show.php) - Show creation, scheduling
- [ ] **Edit Show** (/edit-show.php?id=X) - Show modification

#### **ğŸ”’ Protected Pages (Test Authenticated)**  
- [ ] **Recordings** (/recordings.php) - Audio listings, playback
- [ ] **API Keys** (/settings/api-keys.php) - Key management
- [ ] **Admin Panel** (/admin/dashboard.php) - Admin functions

#### **ğŸ”§ API Endpoints (Test Via Browser)**
- [ ] **Station Discovery** (/api/discover-station.php) - Via Add Station form
- [ ] **RSS Feeds** (/api/enhanced-feeds.php) - Via RSS page links
- [ ] **Test Recording** (/api/test-recording.php) - Via station buttons

## ğŸ“‹ Testing Requirements by Component Type

### **ğŸ›ï¸ Forms and User Input**
**Test ALL of the following:**
- âœ… Form submission with valid data
- âœ… Form validation with invalid/missing data  
- âœ… Error message display and clarity
- âœ… Field validation (required fields, format validation, length limits)
- âœ… CSRF token functionality
- âœ… Success/failure feedback to users
- âœ… Edge cases (empty fields, special characters, very long input)
- âœ… Browser back/forward button behavior
- âœ… Form persistence/clearing behavior

**Example Test Cases:**
```
Station Form Testing:
1. Submit with all fields filled correctly
2. Submit with missing required fields
3. Submit with invalid URL formats
4. Test auto-discovery functionality
5. Test manual entry workflow
6. Verify error messages are helpful and clear
7. Test form reset/cancel functionality
```

### **ğŸ” Search and Filtering**
**Test ALL filter combinations:**
- âœ… Each filter individually
- âœ… Multiple filters combined
- âœ… Filter reset/clear functionality
- âœ… Filter persistence across page reloads
- âœ… Empty result states
- âœ… Search with various query types (partial matches, special characters)
- âœ… Sorting options and order changes
- âœ… Pagination if applicable

**Critical Test Case:**
```
Shows Page Filtering (station_id=3 example):
1. Go to shows page
2. Select "WERU" from station dropdown
3. Click Filter button
4. Verify ONLY WERU shows appear in results
5. Test other filter combinations
6. Verify filter persistence
```

### **ğŸ¨ User Interface Changes**
**Verify ALL interactions:**
- âœ… Hover effects and animations
- âœ… Button click responses and states
- âœ… Modal open/close functionality
- âœ… Responsive design on different screen sizes
- âœ… Loading states and progress indicators
- âœ… Error state displays
- âœ… Accessibility (tab navigation, screen reader compatibility)
- âœ… Cross-browser compatibility (Chrome, Firefox, Safari)

### **ğŸ“¡ API Endpoints and AJAX**
**Test ALL scenarios:**
- âœ… Successful API responses
- âœ… Error responses and handling
- âœ… Network timeout scenarios
- âœ… Invalid input handling
- âœ… Authentication/authorization
- âœ… Rate limiting if applicable
- âœ… Loading states in UI during API calls

### **ğŸ” Authentication and Security**
**Verify security measures:**
- âœ… Login/logout functionality
- âœ… Session timeout behavior
- âœ… CSRF protection
- âœ… Permission-based access control
- âœ… Password validation and requirements
- âœ… Failed login attempt handling

## ğŸ§ª Comprehensive Testing Workflows

### **Before Starting Development**
1. **Understand the Current State**
   - Test existing functionality before making changes
   - Document current behavior
   - Identify potential areas of impact

### **During Development** 
1. **Test Each Component**
   - Implement one feature/fix at a time
   - Test immediately after each change
   - Verify both new and existing functionality
   - Fix issues before proceeding

### **After Implementation**
1. **Complete User Journey Testing**
   - Test the entire user workflow end-to-end
   - Include all relevant user personas and scenarios
   - Test error conditions and edge cases
   - Verify integration with existing features

### **Before Deployment**
1. **Final Verification**
   - Test all changes on production environment
   - Verify no regressions in existing functionality
   - Confirm all requirements are met
   - Document any known limitations or issues

## ğŸ“ Testing Documentation Requirements

### **For Each Feature/Fix**
Document the following:

```markdown
## Testing Performed

### âœ… Test Cases Executed
- [List specific tests performed]
- [Include both positive and negative test cases]
- [Note any edge cases tested]

### ğŸ› Issues Found and Fixed
- [Document any bugs discovered during testing]
- [Explain how each issue was resolved]

### ğŸš€ Production Verification
- [Confirm functionality works on live site]
- [Include specific URLs tested]
- [Note any browser-specific considerations]
```

## ğŸš¨ Common Testing Failures to Avoid

### **âŒ Don't Do This:**
- Assume form validation works without testing invalid inputs
- Test only the "happy path" without error conditions  
- Make changes without verifying on production site
- Test components in isolation without integration testing
- Skip testing of existing functionality after changes
- Rely on code review alone without functional testing

### **âœ… Do This Instead:**
- Test both valid and invalid inputs thoroughly
- Include error conditions and edge cases in testing
- Always verify functionality on the live production site
- Test the complete user workflow from start to finish
- Verify existing features still work after changes
- Combine code review with comprehensive functional testing

## ğŸ¯ Testing Prompt Templates

### **For Feature Development:**
```
Implement [feature description]. 

TESTING REQUIREMENTS:
- Test using the live site at https://radiograb.svaha.com
- Verify complete user workflows end-to-end
- Test error conditions and edge cases
- Confirm existing functionality still works
- Report testing results and any issues found/fixed

Do not proceed to the next task until testing is complete and successful.
```

### **For Bug Fixes:**
```
Fix [bug description].

TESTING REQUIREMENTS:
- Reproduce the exact issue described
- Implement and test the fix
- Verify the fix resolves the original problem
- Test related functionality to ensure no regressions
- Use the live production site for all testing

Confirm the bug is resolved before marking as complete.
```

### **For UI/UX Changes:**
```
Implement [UI changes].

TESTING REQUIREMENTS:
- Test all interactive elements using actual browser
- Verify responsive design on different screen sizes
- Test hover effects, animations, and transitions
- Confirm accessibility and usability
- Test with real user data and scenarios

Ensure the user experience meets requirements before proceeding.
```

## ğŸ”§ Testing Tools and Environment

### **Required Testing Environment:**
- **Live Production Site:** https://radiograb.svaha.com
- **Browser Testing:** Chrome, Firefox, Safari (minimum)
- **Device Testing:** Desktop and mobile viewports
- **Network Conditions:** Test with normal and slow connections

### **Testing Data:**
- Use real production data when possible
- Test with various data states (empty, minimal, extensive)
- Include edge cases in test data (special characters, long text, etc.)

### **Documentation:**
- Keep testing notes and results
- Document any workarounds or known issues
- Update this testing guide based on lessons learned

## ğŸ“Š Quality Gates

### **No deployment without:**
- âœ… All functional requirements tested and working
- âœ… Error conditions handled gracefully
- âœ… User experience verified on live site
- âœ… No regressions in existing functionality
- âœ… Cross-browser compatibility confirmed
- âœ… Mobile responsiveness verified (if applicable)

### **Definition of Done:**
A feature/fix is only complete when:
1. Code is implemented correctly
2. All testing requirements are met
3. Production deployment is successful
4. End-to-end user workflows are verified
5. No critical issues remain unresolved

---

## ğŸ’¡ Remember

**"If it's not tested in production with real user workflows, it's not done."**

Quality software requires comprehensive testing. Taking time to test thoroughly prevents user-facing bugs and maintains system reliability. Every bug caught in testing is a bug that doesn't impact users.