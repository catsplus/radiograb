# Testing Guidelines for RadioGrab Development

## ğŸ¯ Overview

This document establishes comprehensive testing requirements for all RadioGrab development work. Every code change must be thoroughly tested using real user workflows to ensure reliability and quality.

## ğŸš¨ Core Testing Principles

### **1. Always Test as a Real User**
- Use the actual web browser interface at https://radiograb.svaha.com
- Follow complete user workflows from start to finish
- Never assume functionality works - verify it through actual usage
- Test using the same methods and interfaces that end users would use

### **2. Test Every Change**
- Test immediately after implementing each feature/fix
- Don't batch testing - verify each component as you build it
- Test both the new functionality and existing features to ensure no regressions
- If you find issues during testing, fix them before proceeding to the next task

### **3. Production Environment Testing**
- Always test on the live production site (https://radiograb.svaha.com)
- Use the deployed version, not just local development
- Verify changes work in the actual Docker container environment
- Test with real data and real user scenarios

## ğŸ“‹ Testing Requirements by Component Type

### **ğŸ›ï¸ Forms and User Input**
**Test ALL of the following:**
- âœ… Form submission with valid data
- âœ… Form validation with invalid/missing data  
- âœ… Error message display and clarity
- âœ… Field validation (required fields, format validation, length limits)
- âœ… CSRF token functionality
- âœ… Success/failure feedback to users
- âœ… Edge cases (empty fields, special characters, very long input)
- âœ… Browser back/forward button behavior
- âœ… Form persistence/clearing behavior

**Example Test Cases:**
```
Station Form Testing:
1. Submit with all fields filled correctly
2. Submit with missing required fields
3. Submit with invalid URL formats
4. Test auto-discovery functionality
5. Test manual entry workflow
6. Verify error messages are helpful and clear
7. Test form reset/cancel functionality
```

### **ğŸ” Search and Filtering**
**Test ALL filter combinations:**
- âœ… Each filter individually
- âœ… Multiple filters combined
- âœ… Filter reset/clear functionality
- âœ… Filter persistence across page reloads
- âœ… Empty result states
- âœ… Search with various query types (partial matches, special characters)
- âœ… Sorting options and order changes
- âœ… Pagination if applicable

**Critical Test Case:**
```
Shows Page Filtering (station_id=3 example):
1. Go to shows page
2. Select "WERU" from station dropdown
3. Click Filter button
4. Verify ONLY WERU shows appear in results
5. Test other filter combinations
6. Verify filter persistence
```

### **ğŸ¨ User Interface Changes**
**Verify ALL interactions:**
- âœ… Hover effects and animations
- âœ… Button click responses and states
- âœ… Modal open/close functionality
- âœ… Responsive design on different screen sizes
- âœ… Loading states and progress indicators
- âœ… Error state displays
- âœ… Accessibility (tab navigation, screen reader compatibility)
- âœ… Cross-browser compatibility (Chrome, Firefox, Safari)

### **ğŸ“¡ API Endpoints and AJAX**
**Test ALL scenarios:**
- âœ… Successful API responses
- âœ… Error responses and handling
- âœ… Network timeout scenarios
- âœ… Invalid input handling
- âœ… Authentication/authorization
- âœ… Rate limiting if applicable
- âœ… Loading states in UI during API calls

### **ğŸ” Authentication and Security**
**Verify security measures:**
- âœ… Login/logout functionality
- âœ… Session timeout behavior
- âœ… CSRF protection
- âœ… Permission-based access control
- âœ… Password validation and requirements
- âœ… Failed login attempt handling

## ğŸ§ª Comprehensive Testing Workflows

### **Before Starting Development**
1. **Understand the Current State**
   - Test existing functionality before making changes
   - Document current behavior
   - Identify potential areas of impact

### **During Development** 
1. **Test Each Component**
   - Implement one feature/fix at a time
   - Test immediately after each change
   - Verify both new and existing functionality
   - Fix issues before proceeding

### **After Implementation**
1. **Complete User Journey Testing**
   - Test the entire user workflow end-to-end
   - Include all relevant user personas and scenarios
   - Test error conditions and edge cases
   - Verify integration with existing features

### **Before Deployment**
1. **Final Verification**
   - Test all changes on production environment
   - Verify no regressions in existing functionality
   - Confirm all requirements are met
   - Document any known limitations or issues

## ğŸ“ Testing Documentation Requirements

### **For Each Feature/Fix**
Document the following:

```markdown
## Testing Performed

### âœ… Test Cases Executed
- [List specific tests performed]
- [Include both positive and negative test cases]
- [Note any edge cases tested]

### ğŸ› Issues Found and Fixed
- [Document any bugs discovered during testing]
- [Explain how each issue was resolved]

### ğŸš€ Production Verification
- [Confirm functionality works on live site]
- [Include specific URLs tested]
- [Note any browser-specific considerations]
```

## ğŸš¨ Common Testing Failures to Avoid

### **âŒ Don't Do This:**
- Assume form validation works without testing invalid inputs
- Test only the "happy path" without error conditions  
- Make changes without verifying on production site
- Test components in isolation without integration testing
- Skip testing of existing functionality after changes
- Rely on code review alone without functional testing

### **âœ… Do This Instead:**
- Test both valid and invalid inputs thoroughly
- Include error conditions and edge cases in testing
- Always verify functionality on the live production site
- Test the complete user workflow from start to finish
- Verify existing features still work after changes
- Combine code review with comprehensive functional testing

## ğŸ¯ Testing Prompt Templates

### **For Feature Development:**
```
Implement [feature description]. 

TESTING REQUIREMENTS:
- Test using the live site at https://radiograb.svaha.com
- Verify complete user workflows end-to-end
- Test error conditions and edge cases
- Confirm existing functionality still works
- Report testing results and any issues found/fixed

Do not proceed to the next task until testing is complete and successful.
```

### **For Bug Fixes:**
```
Fix [bug description].

TESTING REQUIREMENTS:
- Reproduce the exact issue described
- Implement and test the fix
- Verify the fix resolves the original problem
- Test related functionality to ensure no regressions
- Use the live production site for all testing

Confirm the bug is resolved before marking as complete.
```

### **For UI/UX Changes:**
```
Implement [UI changes].

TESTING REQUIREMENTS:
- Test all interactive elements using actual browser
- Verify responsive design on different screen sizes
- Test hover effects, animations, and transitions
- Confirm accessibility and usability
- Test with real user data and scenarios

Ensure the user experience meets requirements before proceeding.
```

## ğŸ”§ Testing Tools and Environment

### **Required Testing Environment:**
- **Live Production Site:** https://radiograb.svaha.com
- **Browser Testing:** Chrome, Firefox, Safari (minimum)
- **Device Testing:** Desktop and mobile viewports
- **Network Conditions:** Test with normal and slow connections

### **Testing Data:**
- Use real production data when possible
- Test with various data states (empty, minimal, extensive)
- Include edge cases in test data (special characters, long text, etc.)

### **Documentation:**
- Keep testing notes and results
- Document any workarounds or known issues
- Update this testing guide based on lessons learned

## ğŸ“Š Quality Gates

### **No deployment without:**
- âœ… All functional requirements tested and working
- âœ… Error conditions handled gracefully
- âœ… User experience verified on live site
- âœ… No regressions in existing functionality
- âœ… Cross-browser compatibility confirmed
- âœ… Mobile responsiveness verified (if applicable)

### **Definition of Done:**
A feature/fix is only complete when:
1. Code is implemented correctly
2. All testing requirements are met
3. Production deployment is successful
4. End-to-end user workflows are verified
5. No critical issues remain unresolved

---

## ğŸ’¡ Remember

**"If it's not tested in production with real user workflows, it's not done."**

Quality software requires comprehensive testing. Taking time to test thoroughly prevents user-facing bugs and maintains system reliability. Every bug caught in testing is a bug that doesn't impact users.